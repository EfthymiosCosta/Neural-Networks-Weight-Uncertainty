{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BayesCNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNW3/zRr/PV/MqQMETxVkeS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EfthymiosCosta/Neural-Networks-Weight-Uncertainty/blob/main/BayesCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_ArF06Ajx0f"
      },
      "source": [
        "!git clone https://github.com/kumar-shridhar/PyTorch-BayesianCNN.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd-C-EuPk8L2",
        "outputId": "f8114856-616f-4274-8c59-bfc2cb8ca23e"
      },
      "source": [
        "%cd BayesianCNN"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/BayesianCNN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ycosHgyj9NL"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.optim import Adam, lr_scheduler\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import data\n",
        "import utils\n",
        "import metrics\n",
        "import config_bayesian as cfg\n",
        "from BayesianCNN.models.BayesianModels.Bayesian3Conv3FC import BBB3Conv3FC\n",
        "from BayesianCNN.models.BayesianModels.BayesianAlexNet import BBBAlexNet\n",
        "from BayesianCNN.models.BayesianModels.BayesianLeNet import BBBLeNet"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1MStMDzkQFK"
      },
      "source": [
        "# CUDA settings\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def getModel(net_type, inputs, outputs, priors, layer_type, activation_type):\n",
        "    if (net_type == 'lenet'):\n",
        "        return BBBLeNet(outputs, inputs, priors, layer_type, activation_type)\n",
        "    elif (net_type == 'alexnet'):\n",
        "        return BBBAlexNet(outputs, inputs, priors, layer_type, activation_type)\n",
        "    elif (net_type == '3conv3fc'):\n",
        "        return BBB3Conv3FC(outputs, inputs, priors, layer_type, activation_type)\n",
        "    else:\n",
        "        raise ValueError('Network should be either [LeNet / AlexNet / 3Conv3FC')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9quPfIOkRx3"
      },
      "source": [
        "def train_model(net, optimizer, criterion, trainloader, num_ens=1, beta_type=0.1, epoch=None, num_epochs=None):\n",
        "    net.train()\n",
        "    training_loss = 0.0\n",
        "    accs = []\n",
        "    kl_list = []\n",
        "    for i, (inputs, labels) in enumerate(trainloader, 1):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = torch.zeros(inputs.shape[0], net.num_classes, num_ens).to(device)\n",
        "\n",
        "        kl = 0.0\n",
        "        for j in range(num_ens):\n",
        "            net_out, _kl = net(inputs)\n",
        "            kl += _kl\n",
        "            outputs[:, :, j] = F.log_softmax(net_out, dim=1)\n",
        "        \n",
        "        kl = kl / num_ens\n",
        "        kl_list.append(kl.item())\n",
        "        log_outputs = utils.logmeanexp(outputs, dim=2)\n",
        "\n",
        "        beta = metrics.get_beta(i-1, len(trainloader), beta_type, epoch, num_epochs)\n",
        "        loss = criterion(log_outputs, labels, kl, beta)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        accs.append(metrics.acc(log_outputs.data, labels))\n",
        "        training_loss += loss.cpu().data.numpy()\n",
        "    return training_loss/len(trainloader), np.mean(accs), np.mean(kl_list)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_gx6CPNlBhP"
      },
      "source": [
        "def validate_model(net, criterion, validloader, num_ens=1, beta_type=0.1, epoch=None, num_epochs=None):\n",
        "    \"\"\"Calculate ensemble accuracy and NLL Loss\"\"\"\n",
        "    net.train()\n",
        "    valid_loss = 0.0\n",
        "    accs = []\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(validloader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = torch.zeros(inputs.shape[0], net.num_classes, num_ens).to(device)\n",
        "        kl = 0.0\n",
        "        for j in range(num_ens):\n",
        "            net_out, _kl = net(inputs)\n",
        "            kl += _kl\n",
        "            outputs[:, :, j] = F.log_softmax(net_out, dim=1).data\n",
        "\n",
        "        log_outputs = utils.logmeanexp(outputs, dim=2)\n",
        "\n",
        "        beta = metrics.get_beta(i-1, len(validloader), beta_type, epoch, num_epochs)\n",
        "        valid_loss += criterion(log_outputs, labels, kl, beta).item()\n",
        "        accs.append(metrics.acc(log_outputs, labels))\n",
        "\n",
        "    return valid_loss/len(validloader), np.mean(accs)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sFELH0klGsg"
      },
      "source": [
        "def run(dataset, net_type):\n",
        "\n",
        "    # Hyper Parameter settings\n",
        "    layer_type = cfg.layer_type\n",
        "    activation_type = cfg.activation_type\n",
        "    priors = cfg.priors\n",
        "\n",
        "    train_ens = cfg.train_ens\n",
        "    valid_ens = cfg.valid_ens\n",
        "    n_epochs = cfg.n_epochs\n",
        "    lr_start = cfg.lr_start\n",
        "    num_workers = cfg.num_workers\n",
        "    valid_size = cfg.valid_size\n",
        "    batch_size = cfg.batch_size\n",
        "    beta_type = cfg.beta_type\n",
        "\n",
        "    trainset, testset, inputs, outputs = data.getDataset(dataset)\n",
        "    train_loader, valid_loader, test_loader = data.getDataloader(\n",
        "        trainset, testset, valid_size, batch_size, num_workers)\n",
        "    net = getModel(net_type, inputs, outputs, priors, layer_type, activation_type).to(device)\n",
        "\n",
        "    ckpt_dir = f'checkpoints/{dataset}/bayesian'\n",
        "    ckpt_name = f'checkpoints/{dataset}/bayesian/model_{net_type}_{layer_type}_{activation_type}.pt'\n",
        "\n",
        "    if not os.path.exists(ckpt_dir):\n",
        "        os.makedirs(ckpt_dir, exist_ok=True)\n",
        "\n",
        "    criterion = metrics.ELBO(len(trainset)).to(device)\n",
        "    optimizer = Adam(net.parameters(), lr=lr_start)\n",
        "    lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
        "    valid_loss_max = np.Inf\n",
        "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        train_loss, train_acc, train_kl = train_model(net, optimizer, criterion, train_loader, num_ens=train_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
        "        valid_loss, valid_acc = validate_model(net, criterion, valid_loader, num_ens=valid_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
        "        lr_sched.step(valid_loss)\n",
        "\n",
        "        print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.4f} \\ttrain_kl_div: {:.4f}'.format(\n",
        "            epoch, train_loss, train_acc, valid_loss, valid_acc, train_kl))\n",
        "\n",
        "        # save model if validation accuracy has increased\n",
        "        if valid_loss <= valid_loss_max:\n",
        "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "                valid_loss_max, valid_loss))\n",
        "            torch.save(net.state_dict(), ckpt_name)\n",
        "            valid_loss_max = valid_loss"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PC-oRSNXlSHp"
      },
      "source": [
        "dataset = 'MNIST'\n",
        "net_type = 'lenet'"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGtvSnbVlWic",
        "outputId": "c9960799-b60d-477f-c557-b0a8aaf1281c"
      },
      "source": [
        "run(dataset, net_type)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 \tTraining Loss: 937553.2158 \tTraining Accuracy: 0.3458 \tValidation Loss: 605239.3870 \tValidation Accuracy: 0.8162 \ttrain_kl_div: 8298283.7899\n",
            "Validation loss decreased (inf --> 605239.386968).  Saving model ...\n",
            "Epoch: 1 \tTraining Loss: 505356.4390 \tTraining Accuracy: 0.8713 \tValidation Loss: 428331.7154 \tValidation Accuracy: 0.9049 \ttrain_kl_div: 4798783.0000\n",
            "Validation loss decreased (605239.386968 --> 428331.715426).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 377164.9784 \tTraining Accuracy: 0.9109 \tValidation Loss: 331407.4920 \tValidation Accuracy: 0.9241 \ttrain_kl_div: 3594515.5811\n",
            "Validation loss decreased (428331.715426 --> 331407.492021).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 296206.3501 \tTraining Accuracy: 0.9350 \tValidation Loss: 264512.1114 \tValidation Accuracy: 0.9408 \ttrain_kl_div: 2831249.8710\n",
            "Validation loss decreased (331407.492021 --> 264512.111370).  Saving model ...\n",
            "Epoch: 4 \tTraining Loss: 239152.1354 \tTraining Accuracy: 0.9468 \tValidation Loss: 215796.6034 \tValidation Accuracy: 0.9526 \ttrain_kl_div: 2285332.4441\n",
            "Validation loss decreased (264512.111370 --> 215796.603391).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 197062.3792 \tTraining Accuracy: 0.9532 \tValidation Loss: 179036.5711 \tValidation Accuracy: 0.9568 \ttrain_kl_div: 1877545.8145\n",
            "Validation loss decreased (215796.603391 --> 179036.571144).  Saving model ...\n",
            "Epoch: 6 \tTraining Loss: 164654.1080 \tTraining Accuracy: 0.9577 \tValidation Loss: 151934.6699 \tValidation Accuracy: 0.9554 \ttrain_kl_div: 1563245.1695\n",
            "Validation loss decreased (179036.571144 --> 151934.669880).  Saving model ...\n",
            "Epoch: 7 \tTraining Loss: 139207.6144 \tTraining Accuracy: 0.9611 \tValidation Loss: 128111.9877 \tValidation Accuracy: 0.9647 \ttrain_kl_div: 1316713.4169\n",
            "Validation loss decreased (151934.669880 --> 128111.987699).  Saving model ...\n",
            "Epoch: 8 \tTraining Loss: 118965.6095 \tTraining Accuracy: 0.9632 \tValidation Loss: 110275.2709 \tValidation Accuracy: 0.9644 \ttrain_kl_div: 1118374.6267\n",
            "Validation loss decreased (128111.987699 --> 110275.270944).  Saving model ...\n",
            "Epoch: 9 \tTraining Loss: 102282.3929 \tTraining Accuracy: 0.9654 \tValidation Loss: 95588.0688 \tValidation Accuracy: 0.9656 \ttrain_kl_div: 957076.7473\n",
            "Validation loss decreased (110275.270944 --> 95588.068816).  Saving model ...\n",
            "Epoch: 10 \tTraining Loss: 88642.8490 \tTraining Accuracy: 0.9675 \tValidation Loss: 83086.2615 \tValidation Accuracy: 0.9656 \ttrain_kl_div: 824625.1370\n",
            "Validation loss decreased (95588.068816 --> 83086.261469).  Saving model ...\n",
            "Epoch: 11 \tTraining Loss: 77360.7882 \tTraining Accuracy: 0.9682 \tValidation Loss: 72348.5977 \tValidation Accuracy: 0.9725 \ttrain_kl_div: 714166.1041\n",
            "Validation loss decreased (83086.261469 --> 72348.597739).  Saving model ...\n",
            "Epoch: 12 \tTraining Loss: 67815.5201 \tTraining Accuracy: 0.9710 \tValidation Loss: 64686.4680 \tValidation Accuracy: 0.9662 \ttrain_kl_div: 621888.3587\n",
            "Validation loss decreased (72348.597739 --> 64686.468002).  Saving model ...\n",
            "Epoch: 13 \tTraining Loss: 59676.2931 \tTraining Accuracy: 0.9709 \tValidation Loss: 56563.4802 \tValidation Accuracy: 0.9711 \ttrain_kl_div: 543145.8723\n",
            "Validation loss decreased (64686.468002 --> 56563.480219).  Saving model ...\n",
            "Epoch: 14 \tTraining Loss: 52884.8682 \tTraining Accuracy: 0.9727 \tValidation Loss: 50150.8329 \tValidation Accuracy: 0.9721 \ttrain_kl_div: 476169.1024\n",
            "Validation loss decreased (56563.480219 --> 50150.832862).  Saving model ...\n",
            "Epoch: 15 \tTraining Loss: 46812.1508 \tTraining Accuracy: 0.9731 \tValidation Loss: 44413.5263 \tValidation Accuracy: 0.9734 \ttrain_kl_div: 418902.6127\n",
            "Validation loss decreased (50150.832862 --> 44413.526346).  Saving model ...\n",
            "Epoch: 16 \tTraining Loss: 41711.5128 \tTraining Accuracy: 0.9752 \tValidation Loss: 39866.0742 \tValidation Accuracy: 0.9744 \ttrain_kl_div: 369330.4942\n",
            "Validation loss decreased (44413.526346 --> 39866.074219).  Saving model ...\n",
            "Epoch: 17 \tTraining Loss: 37311.1472 \tTraining Accuracy: 0.9748 \tValidation Loss: 36876.2974 \tValidation Accuracy: 0.9694 \ttrain_kl_div: 326462.9904\n",
            "Validation loss decreased (39866.074219 --> 36876.297374).  Saving model ...\n",
            "Epoch: 18 \tTraining Loss: 33518.9803 \tTraining Accuracy: 0.9757 \tValidation Loss: 32577.5124 \tValidation Accuracy: 0.9724 \ttrain_kl_div: 289522.9498\n",
            "Validation loss decreased (36876.297374 --> 32577.512425).  Saving model ...\n",
            "Epoch: 19 \tTraining Loss: 30062.5108 \tTraining Accuracy: 0.9774 \tValidation Loss: 29325.3427 \tValidation Accuracy: 0.9748 \ttrain_kl_div: 256883.1308\n",
            "Validation loss decreased (32577.512425 --> 29325.342711).  Saving model ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40cITRXGnM9G"
      },
      "source": [
        "trainset, testset, inputs, outputs = data.getDataset(dataset)\n",
        "full_train_loader, full_valid_loader, full_test_loader = data.getDataloader(\n",
        "    trainset, testset, valid_size, 60000, num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFQHhqQRmRFE"
      },
      "source": [
        "validate_model(net, criterion, full_train_loader, num_ens=1, beta_type=0.1, epoch=None, num_epochs=None):"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}